\section{Literature Review}

\subsection{Theoretical Foundations}

\subsubsection{Markov Decision Processes}
Reinforcement learning problems are formally modeled as Markov Decision Processes (MDPs), a mathematical framework first introduced by Bellman \cite{bellman1957} for sequential decision-making under uncertainty. An MDP is defined by the tuple $(S, A, P, R, \gamma)$, where $S$ represents the state space, $A$ the action space, $P$ the transition probability function, $R$ the reward function, and $\gamma$ the discount factor \cite{sutton2018reinforcement}.

The fundamental assumption underlying MDPs is the Markov property, which states that the future state depends only on the current state and action, not on the sequence of events that led to the current state.

The objective in an MDP is to find an optimal policy $\pi^*$ that maximizes the expected cumulative discounted reward:

\begin{equation}
\pi^* = \arg\max_\pi \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R_t | \pi\right]
\end{equation}

where $\gamma \in [0,1]$ is the discount factor that determines the relative importance of immediate versus future rewards.

\subsubsection{Value Functions and Bellman Equations}
The solution to MDPs relies on value functions that estimate the expected return from a given state or state-action pair. The state-value function $V^\pi(s)$ represents the expected return when starting from state $s$ and following policy $\pi$:

\begin{equation}
V^\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s\right]
\end{equation}

Similarly, the action-value function $Q^\pi(s,a)$ represents the expected return when starting from state $s$, taking action $a$, and subsequently following policy $\pi$:

\begin{equation}
Q^\pi(s,a) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a\right]
\end{equation}

The Bellman equations provide recursive relationships for these value functions, forming the theoretical foundation for dynamic programming approaches to solving MDPs \cite{bellman1957}.

\subsection{Q-Learning Algorithm}
Q-learning, introduced by Watkins \cite{watkins1992q}, is a model-free reinforcement learning algorithm that learns the optimal action-value function without requiring knowledge of the environment's transition probabilities or reward function. The algorithm directly estimates $Q^*(s,a)$, the optimal action-value function, through temporal difference learning.

The Q-learning update rule combines immediate reward information with estimates of future value:

\begin{equation}
Q(s,a) \leftarrow Q(s,a) + \alpha \left[r + \gamma \max_{a'} Q(s',a') - Q(s,a)\right]
\end{equation}

where $\alpha$ is the learning rate controlling the magnitude of updates.

\subsection{Exploration in Reinforcement Learning}
The exploration-exploitation dilemma is central to reinforcement learning, requiring agents to balance between exploiting current knowledge and exploring potentially better alternatives \cite{thrun1992efficient}. The $\varepsilon$-greedy strategy, employed in this study, provides a simple yet effective approach by selecting random actions with probability $\varepsilon$ and the greedy action otherwise \cite{sutton2018reinforcement}.

\subsection{Educational Applications of Reinforcement Learning}

Gymnasium \cite{gymnasium2023}, maintained by the Farama Foundation, represents the current standard for reinforcement learning environments, providing a comprehensive collection of benchmark problems. However, these environments often require significant programming expertise and familiarity with Python ecosystems, potentially creating barriers for students new to both programming and reinforcement learning concepts.